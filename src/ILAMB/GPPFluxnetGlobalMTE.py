from netCDF4 import Dataset
import numpy as np
import pylab as plt
import ilamblib as il
from constants import convert
import Post as post
from os import stat,environ
from scipy.interpolate import interp1d

class GPPFluxnetGlobalMTE():
    """Confront models with the gross primary productivity (GPP) product
    generated by Fluxnet MTE.
    """
    def __init__(self):
        self.name = "GPPFluxnetGlobalMTE"
        self.path = environ["ILAMB_ROOT"] + "/DATA/gpp/FLUXNET-MTE/derived/"
        try:
            stat(self.path)
        except:
            raise il.MisplacedData("I am looking for data for the %s confrontation here:\n\n%s\n\nBut I cannot find it. Did you download the data? Have you set the ILAMB_ROOT envronment variable?" % (self.name,self.path))
        self.nlat = 360
        self.nlon = 720
        self.data = {}
        self.data["GppMax"] = 0
        self.data["BiasMaxMag"] = 0
        self.metric = {}
        
    def diagnose(self):
        from pylab import subplots
        from Post import GlobalPlot
        print "Diagnosing the %s confrontation..." % self.name

        to,vo,unit,lat,lon = self.getData(output_unit="g m-2 s-1")
        area = il.CellAreas(lat,lon)

        fig,ax = subplots(nrows=3,figsize=(12,18))
        GlobalPlot(lat,lon,area          ,shift=True,ax=ax[0],biome="global.large")
        GlobalPlot(lat,lon,vo.mask[0,...],shift=True,ax=ax[1],biome="global.large")
        area = np.ma.masked_where(vo.mask[0,...],area,copy=False)
        GlobalPlot(lat,lon,area          ,shift=True,ax=ax[2],biome="global.large")
        ax[0].set_title("area from ilamblib.CellAreas")
        ax[1].set_title("gpp mask")
        ax[2].set_title("land areas * gpp mask")
        fig.savefig("land_areas_%s.png" % self.name)
        return

    def getData(self,initial_time=-1e20,final_time=1e20,output_unit=""):
        """Retrieves the confrontation data on the desired time frame and in
        the desired unit.

        Parameters
        ----------
        initial_time : float, optional
            include model results occurring after this time
        final_time : float, optional
            include model results occurring before this time
        output_unit : string, optional
            if specified, will try to convert the units of the variable 
            extract to these units given (see convert in ILAMB.constants)

        Returns
        -------
        t : numpy.ndarray
            a 1D array of times in days since 00:00:00 1/1/1850
        var : numpy.ma.core.MaskedArray
            an array of the extracted variable
        unit : string
            a description of the extracted unit
        """
        # why are these stored as separate netCDF files? Isn't I/O
        # latency worse if these are broken up and I have to build a
        # composite?
        y0   = max(int(initial_time/365.),1982)
        yf   = min(int(  final_time/365.),2005)
        ny   = yf-y0+1; nm = 12*ny
        t    = np.zeros(nm)
        # FIX: if you ever only wanted some of the MTE data, this would fail
        var  = np.ma.zeros((nm,self.nlat,self.nlon))
        unit = ""
        lat,lon = None,None
        for y in range(ny):
            yr = y+1982
            for m in range(12):
                ind   = 12*y+m
                fname = "%s%d/gpp_0.5x0.5_%d%02d.nc" % (self.path,yr,yr,m+1)
                f = Dataset(fname)
                v = f.variables["gpp"]
                t  [ind    ] = v.time
                var[ind,...] = v[...]
                unit = v.units
                if lat is None:
                    lat = f.variables["lat"][...]
                    lon = f.variables["lon"][...]

        # if you asked for a specific unit, try to convert
        # FIX: migrate this to ilamblib
        unit = unit.lower()
        if output_unit is not "":
            try:
                var *= convert["gpp"][output_unit][unit]
                unit = output_unit
            except:
                raise il.UnknownUnit("Variable is in units of [%s], you asked for [%s] but I do not know how to convert" % (unit,output_unit))
        return t,var,unit,lat,lon

    def confront(self,m):
        r"""Confronts the input model with the observational data.

        Parameters
        ----------
        m : ILAMB.ModelResult.ModelResult
            the model results                  

        Returns
        -------
        cdata : dictionary                  
            contains all outputs/metrics

        Notes
        -----
        The dictionary key "metric" will return a dictionary which
        contains the analysis results. For this confrontation we
        include the following quantities in the analysis. We define
        :math:`gpp(\mathbf{x},t)` as the mean monthly gross primary
        productivity as a function of space (:math:`\mathbf{x}`) and
        time (:math:`t`) given in units of "g m-2 s-1". For
        convenience, we will define here a spatially integrated
        quantity as well,
        
        .. math:: \overline{gpp}(t) = \int_A gpp(\mathbf{x},t)\ dA

        where :math:`A` refers to the area of interest.

        "PeriodMean" : float
            The mean gross primary productivity for the globe
            averaged over the time period, or

            .. math:: \frac{\int_{t_0}^{t_f} \overline{gpp}(t)\ dt}{t_f-t_0}

            in units of "g/s"
        "MonthlyMeanBias" : float
            The bias of the spatially integrated monthly mean model
            result compared to that of the observational data in units
            of "g/s"
        "MonthlyMeanRMSE" : float
            The RMSE of the spatially integrated monthly mean model
            result compared to that of the observational data in units
            of "g/s"
        "PhaseChange" : float
            The mean time difference in the annual peaks of gross
            primary productivity in the model result compared to the
            observational data. The annual peak time is written as
            :math:`t_{\text{peak}}(\mathbf{x},t_a)` where :math:`t_a`
            refers to the year. Then we can compute a temporally
            averaged quantity,

            .. math:: \bar{t}_{\text{peak}}(\mathbf{x}) = \frac{1}{t_{af}-t_{a0}}\int_{t_{a0}}^{t_{af}} t_{\text{peak}}(\mathbf{x},t)\ dt

            Then the phase change is given as the difference of peak
            times of the model relative to the observations,
            integrated over the area of interest, or

            .. math:: \frac{1}{A} \int_A  \left(\bar{t}_{\text{peak}}^{\text{model}}(\mathbf{x}) - \bar{t}_{\text{peak}}^{\text{obs}}(\mathbf{x})\right)\ dA

        """
        # If the model data doesn't have areas or land fractions, we
        # can't do area studies.
        if m.cell_areas is None or m.land_fraction is None: 
            raise il.AreasNotInModel("The %s model cannot perform the %s confrontation because it does not have either areas or land fractions" % (m.name,self.name))

        # Get the confronation data, but it might already be stored
        # with the confrontation. This will save parsing the same data
        # for each confrontation.
        if "to" not in self.data.keys():
            to,vo,unit,lat,lon = self.getData(output_unit="g m-2 s-1")
            area = np.ma.masked_where(vo.mask[0,...],il.CellAreas(lat,lon),copy=False)
            self.data["to"]   = to
            self.data["vo"]   = vo
            self.data["unit"] = unit
            self.data["lat"]  = lat
            self.data["lon"]  = lon
            self.data["area"] = area
        else:
            to   = self.data["to"]
            vo   = self.data["vo"]
            unit = self.data["unit"]
            lat  = self.data["lat"]
            lon  = self.data["lon"]
            area = self.data["area"]
        
        # time limits for this confrontation, with a little padding to
        # account for differences in monthly time representations
        t0,tf = to.min(),to.max()

        # extract the time, variable, and unit of the model result
        tm,vm,um = m.extractTimeSeries("gpp",initial_time=t0,final_time=tf,
                                       output_unit="g m-2 s-1")

        # sign conversions vary, if all values are non-positive, flip signs
        if (vm>0).sum() == 0: vm *= -1 

        # not all models properly mask out oceans
        vm = np.ma.masked_where((vm.mask+np.abs(vm)<1e-15)>0,vm,copy=False)

        # update time limits, might be less model data than observations
        t0,tf  = tm.min(),tm.max()
        ndays  = tf-t0
        nyears = ndays/365.

        # observation integration, this might already be done so we
        # will save processing time by referring to it 
        if "vohat" not in self.data.keys():
            vohat = il.TemporallyIntegratedTimeSeries(to,vo)    # [g m-2]
            vobar = il.SpatiallyIntegratedTimeSeries(vo,area)   # [g s-1]
            votot = il.TemporallyIntegratedTimeSeries(to,vobar) # [g    ]
            self.data["vohat"] = vohat
            self.data["vobar"] = vobar
            self.data["votot"] = votot
        else:
            vohat = self.data["vohat"]
            vobar = self.data["vobar"]
            votot = self.data["votot"]

        # model integration
        vmhat = il.TemporallyIntegratedTimeSeries(tm,vm)          # [g m-2]
        vmbar = il.SpatiallyIntegratedTimeSeries(vm,m.land_areas) # [g s-1]
        vmtot = il.TemporallyIntegratedTimeSeries(tm,vmbar)       # [g    ]

        # The models could be on different time scales, if so we will
        # need to do nearest neighbor interpolation.
        if tm.shape[0] != to.shape[0]:
            f = interp1d(tm,vmbar,kind="nearest",assume_sorted=True,bounds_error=False)
            vmbar = np.ma.masked_invalid(f(to))

        # populate dictionary to return
        cdata = {}
        
        # put the extracted model data and manipulations here
        cdata["model"] = {} 
        cdata["model"]["t"]    = tm
        cdata["model"]["vhat"] = vmhat
        cdata["model"]["vbar"] = vmbar

        # make a few function aliases to help readibility
        bias = il.Bias
        rmse = il.RootMeanSquaredError

        # give each time a weight to be used in the weighted averages below
        mw  = il.MonthlyWeights(to)
        spy = 365.*24*3600
 
        self.metric["PeriodMean"] = {}
        self.metric["PeriodMean"]["var"]  = votot*1e-15/nyears
        self.metric["PeriodMean"]["unit"] = "Pg yr-1"
        self.data["GppMax"] = max(self.data["GppMax"],np.ma.max(vmhat)/ndays,np.ma.max(vohat)/ndays)

        # compute metrics
        metric = {}
        metric["PeriodMean"] = {}
        metric["PeriodMean"]["var"]  = vmtot*1e-15/nyears
        metric["PeriodMean"]["unit"] = "Pg yr-1"
        metric["MonthlyMeanBias"] = {}
        metric["MonthlyMeanBias"]["var"]       = bias(vobar,vmbar,weights=mw)*1e-15*spy
        metric["MonthlyMeanBias"]["unit"]      = "Pg yr-1"
        metric["MonthlyMeanBiasScore"] = {}
        metric["MonthlyMeanBiasScore"]["var"]  = bias(vobar,vmbar,weights=mw,normalize="score")
        metric["MonthlyMeanBiasScore"]["unit"] = "-"
        metric["MonthlyMeanRMSE"] = {}
        metric["MonthlyMeanRMSE"]["var"]       = rmse(vobar,vmbar,weights=mw)*1e-15*spy
        metric["MonthlyMeanRMSE"]["unit"]      = "Pg yr-1"
        metric["MonthlyMeanRMSEScore"] = {}
        metric["MonthlyMeanRMSEScore"]["var"]  = rmse(vobar,vmbar,weights=mw,normalize="score")
        metric["MonthlyMeanRMSEScore"]["unit"] = "-"
        cdata["metric"] = metric

        # compare the bias using nearest neighbor interpolation of the observational data
        # FIX: migrate this into ilamblib
        rows    = np.apply_along_axis(np.argmin,1,np.abs(lat[:,np.newaxis]-m.lat))
        cols    = np.apply_along_axis(np.argmin,1,np.abs(((lon<0)*(lon+360)+(lon>=0)*lon)[:,np.newaxis]-m.lon))
        biasmap = (vmhat[np.ix_(rows,cols)] - vohat)/ndays
        cdata["model"]["bias"] = biasmap
        self.data["BiasMaxMag"] = max(self.data["BiasMaxMag"],-np.ma.min(biasmap),np.ma.max(biasmap))

        return cdata

    def plot(self,M):
        """Generate all plots for this confrontation
        """
        # Setup some font sizes in matplotlib
        post.UseLatexPltOptions(10)
        # Produce map plots
        self._mapPeriodMeanGPP()
        for m in M: self._mapPeriodMeanGPP(m=m)
        for m in M: self._mapBias(m)
        """
        # composite time series
        fig,ax = plt.subplots(figsize=(12,5))
        self._timeSeriesMeanGPP(ax=ax)
        for m in M: self._timeSeriesMeanGPP(m=m,ax=ax)
        ax.set_xlabel("Simulation time [$y$]")
        ax.set_ylabel("Gross Primary Productivity (GPP) [$g/(m^2 day)$]")
        fig.tight_layout()
        handles, labels = ax.get_legend_handles_labels()
        lgd = ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1,0.5))
        fig.savefig('gpp.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')
        plt.close()
        """

    def _mapPeriodMeanGPP(self,m=None):
        if m is not None:
            if self.name not in m.confrontations.keys(): return
        w     = 6.8
        fig   = plt.figure(figsize=(w,0.4117647058823529*w)) 
        ax    = fig.add_axes([0.06,0.025,0.88,0.965])
        if m is None:
            ax.set_title("Period Mean Gross Primary Productivity (GPP) of %s $g/(m^2 day)$" % self.name)
            lat,lon = self.data["lat"],self.data["lon"]
            var     = self.data["vohat"]/(self.data["to"].max()-self.data["to"].min())
            fname   = "%s_Benchmark.png" % (self.name)
            shift   = False
        else:
            lat,lon = m.lat,m.lon
            var     = m.confrontations[self.name]["model"]["vhat"]/(self.data["to"].max()-self.data["to"].min())
            ax.set_title("Period Mean Gross Primary Productivity (GPP) of %s $g/(m^2 day)$" % m.name)
            fname = "%s_%s.png" % (self.name,m.name)
            shift = True
        post.GlobalPlot(lat,lon,var,
                        ax    = ax,
                        shift = shift,
                        biome = "global",
                        vmin  = 0,
                        vmax  = self.data["GppMax"],
                        cmap  = "Greens")
        fig.savefig(fname)
        plt.close()


    def _mapBias(self,m):
        if self.name not in m.confrontations.keys(): return
        w     = 6.8
        fig   = plt.figure(figsize=(w,0.4117647058823529*w))
        ax    = fig.add_axes([0.06,0.025,0.88,0.965])
        lat   = self.data["lat"]
        lon   = self.data["lon"]
        var   = m.confrontations[self.name]["model"]["bias"]
        ax.set_title("Gross Primary Productivity (GPP) Bias of %s $g/(m^2 day)$" % m.name)
        post.GlobalPlot(lat,lon,var,
                        ax    =  ax,
                        shift =  False,
                        biome =  "global",
                        vmin  = -self.data["BiasMaxMag"],
                        vmax  =  self.data["BiasMaxMag"],
                        cmap  =  "seismic")
        fig.savefig("%s_%s_Bias.png" % (self.name,m.name))
        plt.close()

    def _timeSeriesMeanGPP(self,m=None,ax=None):
        if m is None:
            t    = self.data["to"]/365.+1850
            vbar = self.data["vobar"]/np.ma.sum(self.data["area"])*24.*3600.
            ax.plot(t,vbar,'-',lw=2,color='k',alpha=0.25,label="obs")
        else:
            if self.name not in m.confrontations.keys(): return                
            t    = m.confrontations[self.name]["model"]["t"]/365.+1850
            vbar = m.confrontations[self.name]["model"]["vbar"]/m.land_area*24.*3600.
            ax.plot(t,vbar,'-',color=m.color,label=m.name)

